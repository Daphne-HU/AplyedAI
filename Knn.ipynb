{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nn by Daphne Annink\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the data\n",
    "Here i import the data like given in the assignment \\\n",
    "When importing the validation data change the date's so it sets the lables for 2001 corectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('dataset1.csv', delimiter=';', usecols=[1,2,3,4,5,6,7], converters={5: lambda s: 0 if s == b\"-1\" else float(s), 7: lambda s: 0 if s == b\"-1\" else float(s)})\n",
    "dates = np.genfromtxt('dataset1.csv', delimiter=';', usecols=[0])\n",
    "labels = []\n",
    "for label in dates:\n",
    "  if label < 20000301:\n",
    "    labels.append('winter')\n",
    "  elif 20000301 <= label < 20000601:\n",
    "    labels.append('lente')\n",
    "  elif 20000601 <= label < 20000901:\n",
    "    labels.append('zomer')\n",
    "  elif 20000901 <= label < 20001201:\n",
    "    labels.append('herfst')\n",
    "  else:\n",
    "    labels.append('winter')\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "validation_data = np.genfromtxt('validation1.csv', delimiter=';', usecols=[1,2,3,4,5,6,7], converters={5: lambda s: 0 if s == b\"-1\" else float(s), 7: lambda s: 0 if s == b\"-1\" else float(s)})\n",
    "validation_dates = np.genfromtxt('validation1.csv', delimiter=';', usecols=[0])\n",
    "validation_labels = []\n",
    "for label in validation_dates:\n",
    "  if label < 20010301:\n",
    "    validation_labels.append('winter')\n",
    "  elif 20010301 <= label < 20010601:\n",
    "    validation_labels.append('lente')\n",
    "  elif 20010601 <= label < 20010901:\n",
    "    validation_labels.append('zomer')\n",
    "  elif 20010901 <= label < 20011201:\n",
    "    validation_labels.append('herfst')\n",
    "  else:\n",
    "    validation_labels.append('winter')\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize data\n",
    "\n",
    "I made use of the sklearn library and use there normilize function,\\\n",
    "The  preprocessing.normalize is standart defined to normilize over each sample so i changed the axis to 0 witch wil then normilize over each feature.\\\n",
    "You can see that for example in the data[1] & data[4] the first feature, it has 37 and 38 as values witch get normilzed to a closely 0.04702635 and 0.04829734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25. 61. 35. 81.  0. 20. 10.]\n",
      "[37. 73. 54. 87.  0.  0.  0.]\n",
      "[61. 84. 64. 96.  0. 64. 45.]\n",
      "[ 38.  64.  43.  94.   0.  82. 109.]\n",
      "[0.03177456 0.02617682 0.02158506 0.02632969 0.         0.03142608\n",
      " 0.01081499]\n",
      "[0.04702635 0.03132636 0.03330266 0.02828004 0.         0.\n",
      " 0.        ]\n",
      "[0.07752994 0.03604677 0.03946982 0.03120556 0.         0.10056344\n",
      " 0.04866745]\n",
      "[0.04829734 0.02746421 0.02651878 0.03055544 0.         0.12884691\n",
      " 0.11788338]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "print(data[2])\n",
    "print(data[3])\n",
    "\n",
    "normilized_data = preprocessing.normalize(data, axis=0)\n",
    "normilized_validation_data = preprocessing.normalize(validation_data, axis=0)\n",
    "\n",
    "print(normilized_data[0])\n",
    "print(normilized_data[1])\n",
    "print(normilized_data[2])\n",
    "print(normilized_data[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nn functions\n",
    "\n",
    "Here i worked out the K-nn function like given in the reader and a function to check the results of the K-nn workings \\\n",
    "A big difference in the K-nn algorythm is how you calculate the distance, this is why i made a seperate calculation function where i can test the results of differen distance mesurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winter\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "def calculate_distance(training_data, classifying_point, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Calculates distances based on the chosen metric.\n",
    "\n",
    "    Parameters:\n",
    "    - training_data: numpy array of shape (n_samples, n_features).\n",
    "    - classifying_point: numpy array of shape (n_features,).\n",
    "    - metric: str, distance metric to use (\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\", \"hamming\").\n",
    "\n",
    "    Returns:\n",
    "    - distances: numpy array of shape (n_samples,), distances to the classifying_point.\n",
    "    \"\"\"\n",
    "    if metric == \"euclidean\": # usefull forStraightforward / continuous data\n",
    "        distances = np.linalg.norm(training_data - classifying_point, axis=1)\n",
    "    elif metric == \"manhattan\": # Grid-like data or reduce influence of \"uitschieters\"\n",
    "        distances = np.sum(np.abs(training_data - classifying_point), axis=1)\n",
    "    elif metric == \"cosine\": # handy for higher dimentions or sparse data\n",
    "        dot_product = np.dot(training_data, classifying_point)\n",
    "        norm_a = np.linalg.norm(training_data, axis=1)\n",
    "        norm_b = np.linalg.norm(classifying_point)\n",
    "        distances = 1 - (dot_product / (norm_a * norm_b))\n",
    "    elif metric == \"mahalanobis\": # looks more at relations between features\n",
    "        cov_matrix = np.cov(training_data, rowvar=False)\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "        diff = training_data - classifying_point\n",
    "        distances = np.sqrt(np.sum(np.dot(diff, inv_cov_matrix) * diff, axis=1))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distance metric: {metric}\")\n",
    "    return distances\n",
    "\n",
    "\n",
    "def k_nearest_neighbours(training_data, training_labels, classifying_point, k, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    k-Nearest Neighbours Algorithm with flexible distance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - training_data: numpy array of shape (n_samples, n_features), feature vectors of training set.\n",
    "    - training_labels: numpy array of shape (n_samples,), class labels of training set.\n",
    "    - classifying_point: numpy array of shape (n_features,), feature vector of the test point to classify.\n",
    "    - k: int, number of nearest neighbors to consider.\n",
    "    - metric: str, distance metric to use (\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\", \"hamming\").\n",
    "\n",
    "    Return:\n",
    "    - closest_point_label: label of the most frequent k closest points.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate distances\n",
    "    distances = calculate_distance(training_data, classifying_point, metric=metric)\n",
    "\n",
    "    # Step 2: Select k closest points\n",
    "    index_of_closest_points = np.argsort(distances)[:k]\n",
    "    closest_points_labels = training_labels[index_of_closest_points]\n",
    "\n",
    "    # Step 3: Determine the most frequent class\n",
    "    most_common = Counter(closest_points_labels).most_common()\n",
    "    tied_classes = np.array([most_common[0][0]])\n",
    "\n",
    "    for item in most_common[1:]:\n",
    "        if item[1] == most_common[0][1]:\n",
    "            tied_classes = np.append(tied_classes, item[0])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # If no ties, return the result\n",
    "    if len(tied_classes) == 1:\n",
    "        return tied_classes[0]\n",
    "\n",
    "    # Resolve tie by checking the closest point within the tied classes\n",
    "    shortest_dist = float('inf')\n",
    "    selected_class = None\n",
    "    for i in range(k):\n",
    "        if closest_points_labels[i] in tied_classes:\n",
    "            if distances[index_of_closest_points[i]] < shortest_dist:\n",
    "                shortest_dist = distances[index_of_closest_points[i]]\n",
    "                selected_class = closest_points_labels[i]\n",
    "\n",
    "    return selected_class\n",
    "\n",
    "\n",
    "print(k_nearest_neighbours(normilized_data, labels, normilized_validation_data[0], 10))\n",
    "\n",
    "\n",
    "    \n",
    "def evaluate_k_nearest_neighbours(training_data, training_labels, validation_data, validation_labels, k, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Evaluate k-Nearest Neighbours Algorithm results\n",
    "\n",
    "    Parameters:\n",
    "    - training_data: numpy array of shape (n_samples, n_features), feature vectors of training set.\n",
    "    - training_labels: numpy array of shape (n_samples,), class labels of training set.\n",
    "    - validation_data: numpy array of shape (n_samples, n_features), feature vectors of validation set.\n",
    "    - validation_labels: numpy array of shape (n_samples,), class labels of validation set.\n",
    "    - k: int, number of nearest neighbors to consider.\n",
    "    - metric: str, distance metric to use (\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\", \"hamming\").\n",
    "\n",
    "    Return:\n",
    "    - error procentage: the % of wrong calasifyed data form the validation set\n",
    "    - predicted_labels: numpy array of shape (n_samples,), with the predicted class labels of validation set.\n",
    "    \"\"\"\n",
    "    error_count = 0\n",
    "    predicted_labels = np.array([])\n",
    "    for i in range(len(validation_data)):\n",
    "        predicted_label = k_nearest_neighbours(training_data, training_labels, validation_data[i], k, metric)\n",
    "        predicted_labels = np.append(predicted_labels, predicted_label)\n",
    "        if predicted_label != validation_labels[i]:\n",
    "            error_count += 1\n",
    "    return (error_count / len(validation_data)) * 100, predicted_labels\n",
    "\n",
    "\n",
    "def evaluate_k_nearest_neighbours_diffrent_k(training_data, training_labels, validation_data, validation_labels, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Evaluate k-Nearest Neighbours Algorithm results\n",
    "\n",
    "    Parameters:\n",
    "    - training_data: numpy array of shape (n_samples, n_features), feature vectors of training set.\n",
    "    - training_labels: numpy array of shape (n_samples,), class labels of training set.\n",
    "    - validation_data: numpy array of shape (n_samples, n_features), feature vectors of validation set.\n",
    "    - validation_labels: numpy array of shape (n_samples,), class labels of validation set.\n",
    "    - metric: str, distance metric to use (\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\", \"hamming\").\n",
    "\n",
    "    Return:\n",
    "    - lowest_error: the lowest error found\n",
    "    - lowest_error_k: the k that coresponds to the lowest error found\n",
    "    \"\"\"\n",
    "    lowest_error = float(\"inf\")\n",
    "    lowest_error_k = 0\n",
    "    best_predicted_lables = np.array([])\n",
    "    for k in range(1, 100):\n",
    "        error, predicted_labels = evaluate_k_nearest_neighbours(normilized_data, labels, normilized_validation_data, validation_labels, k, metric)\n",
    "        if(lowest_error > error):\n",
    "            lowest_error = error\n",
    "            lowest_error_k = k\n",
    "            best_predicted_lables = predicted_labels\n",
    "    return lowest_error, lowest_error_k, best_predicted_lables\n",
    "\n",
    "\n",
    "def log_some_examples(original_labels, given_labels, log_amount):\n",
    "    \"\"\"\n",
    "    Logs a specified number of random examples, showing the original and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - original_labels: List or array of original labels.\n",
    "    - given_labels: List or array of predicted labels.\n",
    "    - log_amount: Number of examples to log.\n",
    "    \"\"\"\n",
    "    random_indices = random.sample(range(len(original_labels)), log_amount)\n",
    "    for idx in random_indices:\n",
    "        print(f\"Index {idx}: season: {original_labels[idx]} predicted season: {given_labels[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assigmet said: \"Express the error as a percentage of the validation-set that was classified incorrectly\" \\\n",
    "this means that the lower the % the better \\\n",
    "\\\n",
    "Here i test the what the best K would be and also what the differences in different distance calculations would be \\\n",
    "Based on the explenation of the working of the different calculations (i noted with the calculation function itself) i am expecting cosine or mahalanobis to work the best becouse we dont have a lot of datapoints but quite a few features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained with euclidean:\n",
      "lowest error: 50.0, with K: 54\n",
      "Index 49: season: lente predicted season: zomer\n",
      "Index 67: season: zomer predicted season: zomer\n",
      "Index 30: season: lente predicted season: herfst\n",
      "Index 48: season: lente predicted season: zomer\n",
      "\n",
      "Trained with manhattan:\n",
      "lowest error: 53.0, with K: 31\n",
      "Index 61: season: zomer predicted season: zomer\n",
      "Index 56: season: zomer predicted season: zomer\n",
      "Index 55: season: zomer predicted season: zomer\n",
      "Index 16: season: winter predicted season: herfst\n",
      "\n",
      "Trained with cosine:\n",
      "lowest error: 41.0, with K: 54\n",
      "Index 42: season: lente predicted season: lente\n",
      "Index 64: season: zomer predicted season: herfst\n",
      "Index 85: season: herfst predicted season: zomer\n",
      "Index 50: season: lente predicted season: zomer\n",
      "\n",
      "Trained with mahalanobis:\n",
      "lowest error: 42.0, with K: 79\n",
      "Index 64: season: zomer predicted season: zomer\n",
      "Index 52: season: zomer predicted season: lente\n",
      "Index 83: season: herfst predicted season: lente\n",
      "Index 86: season: herfst predicted season: zomer\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained with euclidean:\")\n",
    "ecu_error, ecu_error_k, ecu_best_predicted_lables = evaluate_k_nearest_neighbours_diffrent_k(normilized_data, labels, normilized_validation_data, validation_labels, \"euclidean\")\n",
    "print(f\"lowest error: {ecu_error}, with K: {ecu_error_k}\")\n",
    "log_some_examples(validation_labels, ecu_best_predicted_lables, 4)\n",
    "\n",
    "print(\"\\nTrained with manhattan:\")\n",
    "manh_error, manh_error_k, manh_best_predicted_lables = evaluate_k_nearest_neighbours_diffrent_k(normilized_data, labels, normilized_validation_data, validation_labels, \"manhattan\")\n",
    "print(f\"lowest error: {manh_error}, with K: {manh_error_k}\")\n",
    "log_some_examples(validation_labels, manh_best_predicted_lables, 4)\n",
    "\n",
    "print(\"\\nTrained with cosine:\")\n",
    "cos_error, cos_error_k, cos_best_predicted_lables = evaluate_k_nearest_neighbours_diffrent_k(normilized_data, labels, normilized_validation_data, validation_labels, \"cosine\")\n",
    "print(f\"lowest error: {cos_error}, with K: {cos_error_k}\")\n",
    "log_some_examples(validation_labels, cos_best_predicted_lables, 4)\n",
    "\n",
    "print(\"\\nTrained with mahalanobis:\")\n",
    "mah_error, mah_error_k, mah_best_predicted_lables = evaluate_k_nearest_neighbours_diffrent_k(normilized_data, labels, normilized_validation_data, validation_labels, \"mahalanobis\")\n",
    "print(f\"lowest error: {mah_error}, with K: {mah_error_k}\")\n",
    "log_some_examples(validation_labels, mah_best_predicted_lables, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realy cool to see the predictions all togheter like this, like i expected cosine has the lowest error rate followed by mahalanobis \\\n",
    "i like to visualize data so i made a function to also print some random examples of the predicted labes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The lable days exersize\n",
    "Now for the final exersize, i run then with both the best calculation methods and was supprised to see that the mahalanobis only predicted \"lente\"\\\n",
    "while it did predict different oupyts in the last rounds, so i asume this is the output it suppose to be for this calculation \\\n",
    "\\\n",
    "becouse we have not a lot of input data i also tryed to take both the data an validation data combined to have more training data and predict the days again based on this, and again for both calculation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction with cosine and K 54:\n",
      "Day 1: winter\n",
      "Day 2: lente\n",
      "Day 3: herfst\n",
      "Day 4: zomer\n",
      "Day 5: winter\n",
      "Day 6: lente\n",
      "Day 7: winter\n",
      "Day 8: winter\n",
      "Day 9: herfst\n",
      "\n",
      "prediction with mahalanobis and K 79:\n",
      "Day 1: lente\n",
      "Day 2: lente\n",
      "Day 3: lente\n",
      "Day 4: lente\n",
      "Day 5: lente\n",
      "Day 6: lente\n",
      "Day 7: lente\n",
      "Day 8: lente\n",
      "Day 9: lente\n",
      "\n",
      "prediction with combined data cosine and K 54::\n",
      "Day 1: winter\n",
      "Day 2: lente\n",
      "Day 3: herfst\n",
      "Day 4: zomer\n",
      "Day 5: lente\n",
      "Day 6: lente\n",
      "Day 7: winter\n",
      "Day 8: winter\n",
      "Day 9: herfst\n",
      "\n",
      "prediction with combined data mahalanobis and K 79::\n",
      "Day 1: lente\n",
      "Day 2: lente\n",
      "Day 3: lente\n",
      "Day 4: lente\n",
      "Day 5: lente\n",
      "Day 6: lente\n",
      "Day 7: lente\n",
      "Day 8: lente\n",
      "Day 9: lente\n"
     ]
    }
   ],
   "source": [
    "days = np.array([\n",
    "    [40, 52, 2, 102, 103, 0, 0],\n",
    "    [25, 48, -18, 105, 72, 6, 1],\n",
    "    [23, 121, 56, 150, 25, 18, 18],\n",
    "    [27, 229, 146, 308, 130, 0, 0],\n",
    "    [41, 65, 27, 123, 95, 0, 0],\n",
    "    [46, 162, 100, 225, 127, 0, 0],\n",
    "    [23, -27, -41, -16, 0, 0, -1],\n",
    "    [28, -78, -106, -39, 67, 0, 0],\n",
    "    [38, 166, 131, 219, 58, 16, 41]\n",
    "])\n",
    "normalized_days = preprocessing.normalize(days, axis=0)\n",
    "\n",
    "print(\"prediction with cosine and K 54:\")\n",
    "for i, day in enumerate(normalized_days):\n",
    "    label = k_nearest_neighbours(normilized_data, labels, day, 54, \"cosine\")\n",
    "    print(f'Day {i+1}: {label}')\n",
    "\n",
    "print(\"\\nprediction with mahalanobis and K 79:\")\n",
    "for i, day in enumerate(normalized_days):\n",
    "    label = k_nearest_neighbours(normilized_data, labels, day, 79, \"mahalanobis\")\n",
    "    print(f'Day {i+1}: {label}')\n",
    "\n",
    "\n",
    "comib_data = np.vstack((data, validation_data))\n",
    "combi_lables = np.append(labels, validation_labels)\n",
    "normilized_combi_data = preprocessing.normalize(comib_data, axis=0)\n",
    "\n",
    "print(\"\\nprediction with combined data cosine and K 54::\")\n",
    "for i, day in enumerate(normalized_days):\n",
    "    label = k_nearest_neighbours(normilized_combi_data, combi_lables, day, 54, \"cosine\")\n",
    "    print(f'Day {i+1}: {label}')\n",
    "\n",
    "print(\"\\nprediction with combined data mahalanobis and K 79::\")\n",
    "for i, day in enumerate(normalized_days):\n",
    "    label = k_nearest_neighbours(normilized_combi_data, combi_lables, day, 79, \"mahalanobis\")\n",
    "    print(f'Day {i+1}: {label}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HU_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
